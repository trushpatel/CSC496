{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981d9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_path = os.environ['SPARK_HOME']\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.10.9-src.zip\")\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603a789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "import numpy\n",
    "from pyspark.mllib.linalg import Matrix, Matrices, Vectors, DenseMatrix, SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06114368",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_cores = 8\n",
    "memory_gb = 16\n",
    "conf = (pyspark.SparkConf().setMaster('local[{}]'.format(number_cores)).set('spark.driver.memory', '{}g'.format(memory_gb)))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5c6944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_submission.csv  output1.csv  output.csv\ttest.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!dir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d14178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80a694e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('./data/train.csv', header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad2640",
   "metadata": {},
   "source": [
    "- PassengerID\n",
    "- Sex (Women and children first)\n",
    "- Age (Male > 10 considered adult)\n",
    "- Cabin (Numbers increase back to front, front sank first; A-F in decreasing class order)\n",
    "- PClass (higher classes were further away from boiler room/rising water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeedcdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId=1, Survived=0, Pclass=3, Name='Braund, Mr. Owen Harris', Sex='male', Age=22.0, SibSp=1, Parch=0, Ticket='A/5 21171', Fare=7.25, Cabin=None, Embarked='S'),\n",
       " Row(PassengerId=2, Survived=1, Pclass=1, Name='Cumings, Mrs. John Bradley (Florence Briggs Thayer)', Sex='female', Age=38.0, SibSp=1, Parch=0, Ticket='PC 17599', Fare=71.2833, Cabin='C85', Embarked='C'),\n",
       " Row(PassengerId=3, Survived=1, Pclass=3, Name='Heikkinen, Miss. Laina', Sex='female', Age=26.0, SibSp=0, Parch=0, Ticket='STON/O2. 3101282', Fare=7.925, Cabin=None, Embarked='S'),\n",
       " Row(PassengerId=4, Survived=1, Pclass=1, Name='Futrelle, Mrs. Jacques Heath (Lily May Peel)', Sex='female', Age=35.0, SibSp=1, Parch=0, Ticket='113803', Fare=53.1, Cabin='C123', Embarked='S'),\n",
       " Row(PassengerId=5, Survived=0, Pclass=3, Name='Allen, Mr. William Henry', Sex='male', Age=35.0, SibSp=0, Parch=0, Ticket='373450', Fare=8.05, Cabin=None, Embarked='S')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6c55221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f4ea4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql(\"SELECT Survived, PassengerID, Pclass, Sex, Age, Cabin FROM df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de1f9e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------+------+----+-----+\n",
      "|Survived|PassengerID|Pclass|   Sex| Age|Cabin|\n",
      "+--------+-----------+------+------+----+-----+\n",
      "|       0|          1|     3|  male|22.0| null|\n",
      "|       1|          2|     1|female|38.0|  C85|\n",
      "|       1|          3|     3|female|26.0| null|\n",
      "|       1|          4|     1|female|35.0| C123|\n",
      "|       0|          5|     3|  male|35.0| null|\n",
      "|       0|          6|     3|  male|null| null|\n",
      "|       0|          7|     1|  male|54.0|  E46|\n",
      "|       0|          8|     3|  male| 2.0| null|\n",
      "|       1|          9|     3|female|27.0| null|\n",
      "|       1|         10|     2|female|14.0| null|\n",
      "|       1|         11|     3|female| 4.0|   G6|\n",
      "|       1|         12|     1|female|58.0| C103|\n",
      "|       0|         13|     3|  male|20.0| null|\n",
      "|       0|         14|     3|  male|39.0| null|\n",
      "|       0|         15|     3|female|14.0| null|\n",
      "|       1|         16|     2|female|55.0| null|\n",
      "|       0|         17|     3|  male| 2.0| null|\n",
      "|       1|         18|     2|  male|null| null|\n",
      "|       0|         19|     3|female|31.0| null|\n",
      "|       1|         20|     3|female|null| null|\n",
      "+--------+-----------+------+------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a5a47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_survivors = df.select('Survived').distinct().collect()\n",
    "#print(unique_survivors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "954941bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Survived', 'PassengerID', 'Pclass', 'Sex', 'Age', 'Cabin']\n"
     ]
    }
   ],
   "source": [
    "columns = df1.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00cd98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['Survived', 'PassengerID', 'Pclass', 'Sex', 'Age', 'Cabin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0a41a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def myFunction(s):\n",
    "    return (''.join(c for c in s if c.isdigit()) or None, \n",
    "            ''.join(c for c in s if c.isalpha()) or None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ac441",
   "metadata": {},
   "source": [
    "Source: https://stackoverflow.com/questions/21917989/how-to-split-a-python-string-with-numbers-and-letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "005b7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dictionary = {}\n",
    "\n",
    "for c in cat_columns:\n",
    "    unique_c = df1.select(c).distinct().collect()\n",
    "    #print(c + \": \" + str(len(unique_c)))\n",
    "    cat_dictionary[c] = {}\n",
    "    i = 0\n",
    "    for v in unique_c:\n",
    "        if c == \"PassengerID\":\n",
    "            cat_dictionary[c][v[c]] = 0\n",
    "        elif c == \"Sex\":\n",
    "            cat_dictionary[c]['female'] = 5\n",
    "            cat_dictionary[c]['male'] = 1\n",
    "        elif c == \"Age\":\n",
    "            if v[c] == None:\n",
    "                cat_dictionary[c][v[c]] = 0\n",
    "            elif v[c] < 10:\n",
    "                cat_dictionary[c][v[c]] = 5\n",
    "            else:\n",
    "                cat_dictionary[c][v[c]] = 1\n",
    "        elif c == \"Pclass\":\n",
    "            cat_dictionary[c][v[c]] = 3-v[c]\n",
    "        elif c == \"Cabin\":\n",
    "            if v[c] == None:\n",
    "                cat_dictionary[c][v[c]] = 0\n",
    "            else:\n",
    "                s = v[c].split()\n",
    "                for i in s:\n",
    "                    x = myFunction(i)\n",
    "                    if x[0] == None:\n",
    "                        cat_dictionary[c][v[c]] = 0\n",
    "                    elif int(x[0]) < 36 and x[1] == 'A':\n",
    "                        cat_dictionary[c][v[c]] = 3.5\n",
    "                    elif int(x[0]) >= 36 and x[1] == 'A':\n",
    "                        cat_dictionary[c][v[c]] = 2.5\n",
    "                    elif int(x[0]) < 57 and x[1] == 'B':\n",
    "                        cat_dictionary[c][v[c]] = 3\n",
    "                    elif int(x[0]) >= 57 and x[1] == 'B':\n",
    "                        cat_dictionary[c][v[c]] = 2\n",
    "                    elif int(x[0]) < 57 and x[1] == 'C':\n",
    "                        cat_dictionary[c][v[c]] = 3\n",
    "                    elif int(x[0]) >= 57 and x[1] == 'C':\n",
    "                        cat_dictionary[c][v[c]] = 2\n",
    "                    elif int(x[0]) < 51 and x[1] == 'D':\n",
    "                        cat_dictionary[c][v[c]] = 2\n",
    "                    elif int(x[0]) >= 51 and x[1] == 'D':\n",
    "                        cat_dictionary[c][v[c]] = 1\n",
    "                    elif int(x[0]) < 56 and x[1] == 'E':\n",
    "                        cat_dictionary[c][v[c]] = 2\n",
    "                    elif int(x[0]) >= 56 and x[1] == 'E':\n",
    "                        cat_dictionary[c][v[c]] = 1\n",
    "                    elif int(x[0]) < 65 and x[1] == 'F':\n",
    "                        cat_dictionary[c][v[c]] = 0.5\n",
    "                    elif int(x[0]) >= 65 and x[1] == 'F':\n",
    "                        cat_dictionary[c][v[c]] = 1\n",
    "                    elif int(x[0]) < 40 and x[1] == 'G':\n",
    "                        cat_dictionary[c][v[c]] = 0.5\n",
    "                    elif int(x[0]) >= 56 and x[1] == 'G':\n",
    "                        cat_dictionary[c][v[c]] = 1\n",
    "                    else:\n",
    "                        cat_dictionary[c][v[c]] = 0\n",
    "        else:\n",
    "            cat_dictionary[c][v[c]] = i\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d12ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature vector that we will multiply the weights to\n",
    "#print(cat_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bc367df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPrep(r):\n",
    "    key = 0\n",
    "    value = []\n",
    "    for c in columns:\n",
    "        if c == 'Survived':\n",
    "            key = cat_dictionary[c][r[columns.index(c)]]\n",
    "        else:\n",
    "            if c in cat_columns:\n",
    "                value.append(cat_dictionary[c][r[columns.index(c)]])\n",
    "            else:\n",
    "                value.append(r[columns.index(c)])\n",
    "    return LabeledPoint(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc697105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,0.0,1.0,1.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,2.0,5.0,1.0,2.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,5.0,1.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,2.0,5.0,1.0,2.0]),\n",
       " LabeledPoint(1.0, [0.0,0.0,1.0,1.0,0.0])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# values of each feature for each row\n",
    "df_clean = df1.rdd.map(dataPrep)\n",
    "df_clean.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69e5098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns.index(\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bc3d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training set and testing set (in reality you want 3, training, testing, and validating)\n",
    "df_svm = df_clean.randomSplit([0.8, 0.2], 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a49d2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "710\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "print(df_clean.count())\n",
    "print(df_svm[0].count())\n",
    "print(df_svm[1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba0d7b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,0.0,1.0,1.0,0.0])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svm[0].take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6644636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_titanic = SVMWithSGD.train(df_svm[0], iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90f5730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm_titanic.predict([0.0,1.0,1.0,48.0,45.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35709be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testPrediction(p):\n",
    "    prediction = svm_titanic.predict(p.features)\n",
    "    if prediction == p.label:\n",
    "        return (\"correct\", 1)\n",
    "    else:\n",
    "        return (\"incorrect\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c723f54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('correct', 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPrediction(LabeledPoint(1.0, [0.0,0.0,1.0,1.0,0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afeb484d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('correct', 481), ('incorrect', 229)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = df_svm[0].map(testPrediction).reduceByKey(lambda x, y: x + y)\n",
    "df_results.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f9389c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6774647887323944"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "481/(481+229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "078de80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(p):\n",
    "    prediction = svm_titanic.predict(p.features)\n",
    "    return ('-', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5515c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_svm[0].map(lambda x: output(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a601439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66eae904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out1 = spark.createDataFrame(df_out, ['arb','result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81c5c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out1.registerTempTable(\"df_out1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fdac7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arb: string (nullable = true)\n",
      " |-- result: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d69e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = spark.sql(\"SELECT result FROM df_out1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a23167f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ebfcf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.registerTempTable(\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11e85899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "131d0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "new_df = out.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9060006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.registerTempTable(\"new_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72685a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52a9bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rdd = new_df.rdd.map(lambda x: (x[1]+1, x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d75d559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05130a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = spark.createDataFrame(output_rdd, ['PassengerID', 'Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e167c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90af1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.write.csv('output1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8b7549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1 = spark.read.csv('./data/test.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "22d1c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1.registerTempTable('df_test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "63487602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73874540",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.sql(\"SELECT PassengerID, Pclass, Sex, Age, Cabin FROM df_test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1e8ecea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerID=892, Pclass=3, Sex='male', Age=34.5, Cabin=None),\n",
       " Row(PassengerID=893, Pclass=3, Sex='female', Age=47.0, Cabin=None),\n",
       " Row(PassengerID=894, Pclass=2, Sex='male', Age=62.0, Cabin=None),\n",
       " Row(PassengerID=895, Pclass=3, Sex='male', Age=27.0, Cabin=None),\n",
       " Row(PassengerID=896, Pclass=3, Sex='female', Age=22.0, Cabin=None),\n",
       " Row(PassengerID=897, Pclass=3, Sex='male', Age=14.0, Cabin=None),\n",
       " Row(PassengerID=898, Pclass=3, Sex='female', Age=30.0, Cabin=None),\n",
       " Row(PassengerID=899, Pclass=2, Sex='male', Age=26.0, Cabin=None),\n",
       " Row(PassengerID=900, Pclass=3, Sex='female', Age=18.0, Cabin=None),\n",
       " Row(PassengerID=901, Pclass=3, Sex='male', Age=21.0, Cabin=None)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9f3061e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['PassengerID', 'Pclass', 'Sex', 'Age', 'Cabin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7a3713c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dictionary = {}\n",
    "\n",
    "for c in cat_columns:\n",
    "    unique_c = df1.select(c).distinct().collect()\n",
    "    #print(c + \": \" + str(len(unique_c)))\n",
    "    cat_dictionary[c] = {}\n",
    "    i = 0\n",
    "    for v in unique_c:\n",
    "        if c == \"PassengerID\":\n",
    "            cat_dictionary[c][v[c]] = 0\n",
    "        elif c == \"Sex\":\n",
    "            cat_dictionary[c]['female'] = 5\n",
    "            cat_dictionary[c]['male'] = 1\n",
    "        elif c == \"Age\":\n",
    "            if v[c] == None:\n",
    "                cat_dictionary[c][v[c]] = 0\n",
    "            elif v[c] < 10:\n",
    "                cat_dictionary[c][v[c]] = 5\n",
    "            else:\n",
    "                cat_dictionary[c][v[c]] = 1\n",
    "        elif c == \"Pclass\":\n",
    "            cat_dictionary[c][v[c]] = 3-v[c]\n",
    "        elif c == \"Cabin\":\n",
    "            if v[c] == None:\n",
    "                cat_dictionary[c][v[c]] = 0\n",
    "            else:\n",
    "                s = v[c].split()\n",
    "                for i in s:\n",
    "                    x = myFunction(i)\n",
    "                    if x[0] == None:\n",
    "                        cat_dictionary[c][v[c]] = 0\n",
    "                    elif int(x[0]) < 36 and x[1] == 'A':\n",
    "                        cat_dictionary[c][v[c]] = 3.5\n",
    "                    elif int(x[0]) >= 36 and x[1] == 'A':\n",
    "                        cat_dictionary[c][v[c]] = 2.5\n",
    "                    elif int(x[0]) < 57 and x[1] == 'B':\n",
    "                        cat_dictionary[c][v[c]] = 3\n",
    "                    elif int(x[0]) >= 57 and x[1] == 'B':\n",
    "                        cat_dictionary[c][v[c]] = 2\n",
    "                    elif int(x[0]) < 57 and x[1] == 'C':\n",
    "                        cat_dictionary[c][v[c]] = 3\n",
    "                    elif int(x[0]) >= 57 and x[1] == 'C':\n",
    "                        cat_dictionary[c][v[c]] = 2\n",
    "                    elif int(x[0]) < 51 and x[1] == 'D':\n",
    "                        cat_dictionary[c][v[c]] = 2\n",
    "                    elif int(x[0]) >= 51 and x[1] == 'D':\n",
    "                        cat_dictionary[c][v[c]] = 1\n",
    "                    elif int(x[0]) < 56 and x[1] == 'E':\n",
    "                        cat_dictionary[c][v[c]] = 2\n",
    "                    elif int(x[0]) >= 56 and x[1] == 'E':\n",
    "                        cat_dictionary[c][v[c]] = 1\n",
    "                    elif int(x[0]) < 65 and x[1] == 'F':\n",
    "                        cat_dictionary[c][v[c]] = 0.5\n",
    "                    elif int(x[0]) >= 65 and x[1] == 'F':\n",
    "                        cat_dictionary[c][v[c]] = 1\n",
    "                    elif int(x[0]) < 40 and x[1] == 'G':\n",
    "                        cat_dictionary[c][v[c]] = 0.5\n",
    "                    elif int(x[0]) >= 56 and x[1] == 'G':\n",
    "                        cat_dictionary[c][v[c]] = 1\n",
    "                    else:\n",
    "                        cat_dictionary[c][v[c]] = 0\n",
    "        else:\n",
    "            cat_dictionary[c][v[c]] = i\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e7648aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testDataPrep(r):\n",
    "    value = []\n",
    "    for c in columns:\n",
    "        if c in cat_columns:\n",
    "            value.append(cat_dictionary[c][r[columns.index(c)]])\n",
    "        else:\n",
    "            value.append(r[columns.index(c)])\n",
    "    return LabeledPoint(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "30468d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_clean = df_test.rdd.map(testDataPrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8f727024",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 291.0 failed 1 times, most recent failure: Lost task 0.0 in stage 291.0 (TID 5465, pcvm606-1.emulab.net, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-99-e7465eb46516>\", line 5, in testDataPrep\nKeyError: 'male'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-99-e7465eb46516>\", line 5, in testDataPrep\nKeyError: 'male'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-207a414f8c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 291.0 failed 1 times, most recent failure: Lost task 0.0 in stage 291.0 (TID 5465, pcvm606-1.emulab.net, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-99-e7465eb46516>\", line 5, in testDataPrep\nKeyError: 'male'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-99-e7465eb46516>\", line 5, in testDataPrep\nKeyError: 'male'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df_test_clean.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b161cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ea843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
