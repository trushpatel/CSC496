{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018a3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_path = os.environ['SPARK_HOME']\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.10.9-src.zip\")\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee03505",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_cores = 6\n",
    "memory_gb = 16\n",
    "conf = (pyspark.SparkConf().setMaster('local[{}]'.format(number_cores)).set('spark.driver.memory', '{}g'.format(memory_gb)))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e559ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "import numpy\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ecf4a",
   "metadata": {},
   "source": [
    "Run pip install numpy in pyspark environment, restart this notebook and rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da77ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+---+--------------------+--------------------+\n",
      "| id|            features|              hashes|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|(6,[0,1,2],[1.0,1...|[[4.42744551E8], ...|\n",
      "|  1|(6,[2,3,4],[1.0,1...|[[4.48942786E8], ...|\n",
      "|  2|(6,[0,2,4],[1.0,1...|[[1.458682805E9],...|\n",
      "|  3|(6,[1,3,5],[1.0,1...|[[4.42744551E8], ...|\n",
      "|  4|(6,[2,3,5],[1.0,1...|[[4.48942786E8], ...|\n",
      "|  5|(6,[1,2,4],[1.0,1...|[[4.42744551E8], ...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataA = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n",
    "         (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n",
    "         (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),),\n",
    "         (3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "         (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "         (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),),]\n",
    "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=3)\n",
    "model = mh.fit(dfA)\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(dfA).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6644b646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasetA: struct (nullable = false)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- features: vector (nullable = true)\n",
      " |    |-- hashes: array (nullable = true)\n",
      " |    |    |-- element: vector (containsNull = true)\n",
      " |-- datasetB: struct (nullable = false)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- features: vector (nullable = true)\n",
      " |    |-- hashes: array (nullable = true)\n",
      " |    |    |-- element: vector (containsNull = true)\n",
      " |-- distance: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = model.approxSimilarityJoin(dfA, dfA, 1.0, distCol=\"distance\")\n",
    "final.createOrReplaceTempView(\"final\")\n",
    "final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644dcaf5",
   "metadata": {},
   "source": [
    "## Important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87d1acbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+--------------------+--------+\n",
      "| id|            features| id|            features|distance|\n",
      "+---+--------------------+---+--------------------+--------+\n",
      "|  0|(6,[0,1,2],[1.0,1...|  2|(6,[0,2,4],[1.0,1...|     0.5|\n",
      "|  0|(6,[0,1,2],[1.0,1...|  3|(6,[1,3,5],[1.0,1...|     0.8|\n",
      "|  0|(6,[0,1,2],[1.0,1...|  5|(6,[1,2,4],[1.0,1...|     0.5|\n",
      "|  1|(6,[2,3,4],[1.0,1...|  2|(6,[0,2,4],[1.0,1...|     0.5|\n",
      "|  1|(6,[2,3,4],[1.0,1...|  4|(6,[2,3,5],[1.0,1...|     0.5|\n",
      "|  1|(6,[2,3,4],[1.0,1...|  5|(6,[1,2,4],[1.0,1...|     0.5|\n",
      "|  2|(6,[0,2,4],[1.0,1...|  0|(6,[0,1,2],[1.0,1...|     0.5|\n",
      "|  2|(6,[0,2,4],[1.0,1...|  1|(6,[2,3,4],[1.0,1...|     0.5|\n",
      "|  2|(6,[0,2,4],[1.0,1...|  5|(6,[1,2,4],[1.0,1...|     0.5|\n",
      "|  3|(6,[1,3,5],[1.0,1...|  0|(6,[0,1,2],[1.0,1...|     0.8|\n",
      "|  3|(6,[1,3,5],[1.0,1...|  4|(6,[2,3,5],[1.0,1...|     0.5|\n",
      "|  3|(6,[1,3,5],[1.0,1...|  5|(6,[1,2,4],[1.0,1...|     0.8|\n",
      "|  4|(6,[2,3,5],[1.0,1...|  1|(6,[2,3,4],[1.0,1...|     0.5|\n",
      "|  4|(6,[2,3,5],[1.0,1...|  3|(6,[1,3,5],[1.0,1...|     0.5|\n",
      "|  5|(6,[1,2,4],[1.0,1...|  0|(6,[0,1,2],[1.0,1...|     0.5|\n",
      "|  5|(6,[1,2,4],[1.0,1...|  1|(6,[2,3,4],[1.0,1...|     0.5|\n",
      "|  5|(6,[1,2,4],[1.0,1...|  2|(6,[0,2,4],[1.0,1...|     0.5|\n",
      "|  5|(6,[1,2,4],[1.0,1...|  3|(6,[1,3,5],[1.0,1...|     0.8|\n",
      "+---+--------------------+---+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "select datasetA.id, datasetA.features, datasetB.id, datasetB.features, distance\n",
    "from final where datasetA.id != datasetB.id order by datasetA.id, datasetB.id\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12129309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
