{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018a3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_path = os.environ['SPARK_HOME']\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.10.9-src.zip\")\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee03505",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_cores = 6\n",
    "memory_gb = 16\n",
    "conf = (pyspark.SparkConf().setMaster('local[{}]'.format(number_cores)).set('spark.driver.memory', '{}g'.format(memory_gb)))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e559ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "import numpy\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d863f",
   "metadata": {},
   "source": [
    "Run pip install -y numpy in pyspark environment, restart this notebook and rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0c8590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n",
    "         (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n",
    "         (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),),\n",
    "         (3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "         (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "         (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),),]\n",
    "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=3)\n",
    "model = mh.fit(dfA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61066457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
